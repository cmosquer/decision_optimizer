{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB)\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.0.3\n",
      "    Uninstalling pip-19.0.3:\n",
      "      Successfully uninstalled pip-19.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\USUARI~1\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-40kq1pve\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement scikit-learn==1.0.1 (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\n",
      "ERROR: No matching distribution found for scikit-learn==1.0.1\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\usuariohi\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CalibrationDisplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fef9066f063d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSplitter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvaluator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestSetting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTestSetting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Candelaria\\Experimentos\\Repos\\decision_optimizer\\decision_optimizer\\TestSetting.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPlotter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPlotter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvaluator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalibrator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Candelaria\\Experimentos\\Repos\\decision_optimizer\\decision_optimizer\\Plotter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalibration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalibration_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCalibrationDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecision_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvaluator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CalibrationDisplay'"
     ]
    }
   ],
   "source": [
    "from decision_optimizer.Calibrator import *\n",
    "from decision_optimizer.Splitter import Splitter\n",
    "from decision_optimizer.Evaluator import Evaluator\n",
    "from decision_optimizer.TestSetting import TestSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_column = \"GT\"\n",
    "scores_csv = \"decision_optimizer/scores_and_gt.csv\"\n",
    "output_dir = f\"output_evaluation/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure scorers\n",
    "evaluated_score_column = 'lung_normalized_score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # --- Configure test settings & expected scenarios ----#\n",
    "        scenarios_cost_false_positive = [1]\n",
    "        scenarios_cost_false_negative = [1]\n",
    "        scenarios_expected_positive_prior = [0.5]\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        parameters = {}\n",
    "        for runcsv in run_csvs:\n",
    "            run = runcsv.replace('.csv','')\n",
    "            parameters[run] = {}\n",
    "            print(runcsv)\n",
    "            joined_df_train = pd.read_csv(training_csvs + runcsv)\n",
    "            joined_df_train.columns = ['GT', 'Malignancy', 'subset']\n",
    "            joined_df_train.GT = joined_df_train.GT.astype(int)\n",
    "\n",
    "            joined_df_test = pd.read_csv(tests_csvs + runcsv)\n",
    "            joined_df_test.columns = ['id', 'GT', 'Malignancy', 'subset']\n",
    "            joined_df_test.GT = joined_df_test.GT.astype(int)\n",
    "\n",
    "            # -----------Prepare predefined expected scenarios----------------------#\n",
    "            predefined_scenarios = []\n",
    "            for cost_false_positive, cost_false_negative, expected_positive_prior in zip(scenarios_cost_false_positive,\n",
    "                                                                                         scenarios_cost_false_negative,\n",
    "                                                                                         scenarios_expected_positive_prior):\n",
    "                predefined_theta = np.log(\n",
    "                    (cost_false_positive / cost_false_negative) * ((1 - expected_positive_prior) / expected_positive_prior))\n",
    "                effective_positive_prior = expit(-predefined_theta)\n",
    "\n",
    "                predefined_scenarios.append({\n",
    "                    'expected_positive_prior': expected_positive_prior,\n",
    "                    'cost_false_positive': cost_false_positive,\n",
    "                    'cost_false_negative': cost_false_negative,\n",
    "                    'effective_positive_prior': effective_positive_prior,\n",
    "                    'predefined_theta': predefined_theta\n",
    "                })\n",
    "\n",
    "            # ---------------Evaluate multiple test settings & expected scenarios ---------------------------#\n",
    "            all_test_results = pd.DataFrame()\n",
    "            all_train_results = pd.DataFrame()\n",
    "            for k_score, evaluated_score_column in enumerate(evaluated_score_columns):\n",
    "                for k_scenario, current_scenario in enumerate(predefined_scenarios):\n",
    "                    for k_subgroup, subgroup in enumerate(evaluated_subgroups):\n",
    "                        print(subgroup.upper())\n",
    "                        calibrators_dict = {\n",
    "                            'LOG-REG': LogisticRegressionWCECalibrator()\n",
    "                        }\n",
    "\n",
    "                        train_test_output_dir = f'/{output_dir}/validation_plots/{runcsv.replace(\".csv\", \"/\")}'\n",
    "                        os.makedirs(train_test_output_dir, exist_ok=True)\n",
    "                        train_test_setting = TestSetting(joined_df_train,\n",
    "                                                         evaluated_score_column,\n",
    "                                                         ground_truth_column,\n",
    "                                                         train_test_output_dir)\n",
    "                        train_test_setting.set_current_expected_scenario(current_scenario)\n",
    "                        train_test_setting.initial_evaluation()\n",
    "\n",
    "                        if subgroup == 'all':\n",
    "                            joined_df_test_subgroup = joined_df_test\n",
    "                        else:\n",
    "                            joined_df_test_subgroup = joined_df_test[joined_df_test.subset == subgroup]\n",
    "                        print('Subgroup samples in test ', len(joined_df_test_subgroup))\n",
    "                        os.makedirs(f'/{output_dir}/test_plots/{subgroup}/', exist_ok=True)\n",
    "                        test_output_dir = f'/{output_dir}/test_plots/{subgroup}/{runcsv.replace(\".csv\", \"/\")}'\n",
    "                        os.makedirs(test_output_dir, exist_ok=True)\n",
    "                        test_setting = TestSetting(joined_df_test_subgroup,\n",
    "                                                   evaluated_score_column,\n",
    "                                                   ground_truth_column,\n",
    "                                                   test_output_dir)\n",
    "                        test_setting.set_current_expected_scenario(current_scenario)\n",
    "                        test_setting.initial_evaluation()\n",
    "\n",
    "\n",
    "                        # Fit calibrators with train subset\n",
    "                        for calibrator_name, calibrator in calibrators_dict.items():\n",
    "                            # Fit calibrators. For logReg, use effective_positive_priors to compute LLRs and to fit algorithm\n",
    "                            calibrator.train(scores=joined_df_train[evaluated_score_column].values,\n",
    "                                             labels=joined_df_train[ground_truth_column].values,\n",
    "                                             effective_positive_prior=0.5)\n",
    "\n",
    "                            # Evaluate effect on the same training set\n",
    "                            train_test_setting.set_current_expected_scenario(current_scenario)\n",
    "                            train_test_setting.calibration_evaluation(calibrator_name, calibrator)\n",
    "\n",
    "                            # Evaluate effect on the test set\n",
    "                            test_setting.set_current_expected_scenario(current_scenario)\n",
    "                            test_setting.calibration_evaluation(calibrator_name, calibrator, get_posteriors_thresholds=True)\n",
    "\n",
    "                        if subgroup == 'all':\n",
    "                            test_csv_dir = f'/{output_dir}/Runs_Test_with_calibration/{runcsv}'\n",
    "                            test_setting.df.to_csv(test_csv_dir)\n",
    "                            train_test_setting.results['N'] = [len(joined_df_train)] * len(train_test_setting.results)\n",
    "                            train_test_setting.results['N_positive'] = [len(\n",
    "                                joined_df_train[joined_df_train[ground_truth_column] == 1])] * len(\n",
    "                                train_test_setting.results)\n",
    "                            train_csv_dir = f'/{output_dir}/Runs_Validation_with_calibration/{runcsv}'\n",
    "                            train_test_setting.df.to_csv(train_csv_dir)\n",
    "                            all_train_results = all_train_results.append(train_test_setting.results, ignore_index=True)\n",
    "\n",
    "                            parameters[run].update(test_setting.calibrators_parameters)\n",
    "                        test_setting.results['subset'] = [subgroup]*len(test_setting.results)\n",
    "                        test_setting.results['N'] = [len(joined_df_test_subgroup)]*len(test_setting.results)\n",
    "                        test_setting.results['N_positive'] = [len(joined_df_test_subgroup[joined_df_test_subgroup[ground_truth_column] == 1])]*len(test_setting.results)\n",
    "                        all_test_results = all_test_results.append(test_setting.results, ignore_index=True)\n",
    "\n",
    "            all_test_results['run'] = [runcsv.replace('.csv', '')]*len(all_test_results)\n",
    "            if os.path.exists(f'/{output_dir}/test_metrics.csv'):\n",
    "                existing_results = pd.read_csv(f'/{output_dir}/test_metrics.csv')\n",
    "                existing_results = existing_results.append(all_test_results)\n",
    "                existing_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n",
    "            else:\n",
    "                all_test_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n",
    "\n",
    "            all_train_results['run'] = [runcsv.replace('.csv', '')]*len(all_train_results)\n",
    "            if os.path.exists(f'/{output_dir}/validation_metrics.csv'):\n",
    "                existing_results_train = pd.read_csv(f'/{output_dir}/validation_metrics.csv')\n",
    "                existing_results_train = existing_results_train.append(all_train_results)\n",
    "                existing_results_train.to_csv(f'/{output_dir}/validation_metrics.csv', index=False)\n",
    "            else:\n",
    "                all_train_results.to_csv(f'/{output_dir}/validation_metrics.csv', index=False)\n",
    "\n",
    "        try:\n",
    "            with open(f'/{output_dir}/parameters_results.json', \"w\") as fp:\n",
    "                json.dump(parameters, fp)\n",
    "        except Exception as e:\n",
    "            print('Error saving parameter json', e)\n",
    "\n",
    "        #Calculate deltas with perfect calibrated version\n",
    "        all_test_results = pd.read_csv(f'/{output_dir}/test_metrics.csv')\n",
    "        for run in all_test_results.run.unique():\n",
    "            print(run)\n",
    "            for subgroup in all_test_results.subset.unique():\n",
    "                perfect_run = all_test_results[(all_test_results.run == run) &\n",
    "                                               (all_test_results.calibrator == 'perfect_PAV') &\n",
    "                                               (all_test_results.subset == subgroup)]\n",
    "\n",
    "                for metric in ['CE', 'Balanced_CE', 'Brier', 'Balanced_Brier', 'predefined_cost']:\n",
    "                    perfect_metric = perfect_run[metric].values[0]\n",
    "                    for method in ['no_calibration', 'LOG-REG']:\n",
    "                        method_metric = all_test_results[(all_test_results.run == run) &\n",
    "                                                         (all_test_results.calibrator == method) &\n",
    "                                                         (all_test_results.subset == subgroup)][metric].values[0]\n",
    "                        all_test_results.at[(all_test_results['run'] == run) &\n",
    "                                            (all_test_results.calibrator == method) &\n",
    "                                            (all_test_results.subset == subgroup),\n",
    "                                            'delta_' + metric] = method_metric - perfect_metric\n",
    "        all_test_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
