{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_optimizer.Calibrator import *\n",
    "from decision_optimizer.Splitter import Splitter\n",
    "from decision_optimizer.Evaluator import Evaluator\n",
    "from decision_optimizer.TestSetting import TestSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_column = \"GT\"\n",
    "scores_csv = \"decision_optimizer/scores_and_gt.csv\"\n",
    "output_dir = f\"output_evaluation/\"\n",
    "os.makedirs(f'/{output_dir}/Runs_Test_with_calibration/', exist_ok=True)\n",
    "os.makedirs(f'/{output_dir}/Runs_Validation_with_calibration/', exist_ok=True)\n",
    "\n",
    "# Configure scorers\n",
    "evaluated_score_columns = ['Malignancy']\n",
    "evaluated_subgroups = ['all', 'light', 'dark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # --- Configure test settings & expected scenarios ----#\n",
    "        scenarios_cost_false_positive = [1]\n",
    "        scenarios_cost_false_negative = [1]\n",
    "        scenarios_expected_positive_prior = [0.5]\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        parameters = {}\n",
    "        for runcsv in run_csvs:\n",
    "            run = runcsv.replace('.csv','')\n",
    "            parameters[run] = {}\n",
    "            print(runcsv)\n",
    "            joined_df_train = pd.read_csv(training_csvs + runcsv)\n",
    "            joined_df_train.columns = ['GT', 'Malignancy', 'subset']\n",
    "            joined_df_train.GT = joined_df_train.GT.astype(int)\n",
    "\n",
    "            joined_df_test = pd.read_csv(tests_csvs + runcsv)\n",
    "            joined_df_test.columns = ['id', 'GT', 'Malignancy', 'subset']\n",
    "            joined_df_test.GT = joined_df_test.GT.astype(int)\n",
    "\n",
    "            # -----------Prepare predefined expected scenarios----------------------#\n",
    "            predefined_scenarios = []\n",
    "            for cost_false_positive, cost_false_negative, expected_positive_prior in zip(scenarios_cost_false_positive,\n",
    "                                                                                         scenarios_cost_false_negative,\n",
    "                                                                                         scenarios_expected_positive_prior):\n",
    "                predefined_theta = np.log(\n",
    "                    (cost_false_positive / cost_false_negative) * ((1 - expected_positive_prior) / expected_positive_prior))\n",
    "                effective_positive_prior = expit(-predefined_theta)\n",
    "\n",
    "                predefined_scenarios.append({\n",
    "                    'expected_positive_prior': expected_positive_prior,\n",
    "                    'cost_false_positive': cost_false_positive,\n",
    "                    'cost_false_negative': cost_false_negative,\n",
    "                    'effective_positive_prior': effective_positive_prior,\n",
    "                    'predefined_theta': predefined_theta\n",
    "                })\n",
    "\n",
    "            # ---------------Evaluate multiple test settings & expected scenarios ---------------------------#\n",
    "            all_test_results = pd.DataFrame()\n",
    "            all_train_results = pd.DataFrame()\n",
    "            for k_score, evaluated_score_column in enumerate(evaluated_score_columns):\n",
    "                for k_scenario, current_scenario in enumerate(predefined_scenarios):\n",
    "                    for k_subgroup, subgroup in enumerate(evaluated_subgroups):\n",
    "                        print(subgroup.upper())\n",
    "                        calibrators_dict = {\n",
    "                            'LOG-REG': LogisticRegressionWCECalibrator()\n",
    "                        }\n",
    "\n",
    "                        train_test_output_dir = f'/{output_dir}/validation_plots/{runcsv.replace(\".csv\", \"/\")}'\n",
    "                        os.makedirs(train_test_output_dir, exist_ok=True)\n",
    "                        train_test_setting = TestSetting(joined_df_train,\n",
    "                                                         evaluated_score_column,\n",
    "                                                         ground_truth_column,\n",
    "                                                         train_test_output_dir)\n",
    "                        train_test_setting.set_current_expected_scenario(current_scenario)\n",
    "                        train_test_setting.initial_evaluation()\n",
    "\n",
    "                        if subgroup == 'all':\n",
    "                            joined_df_test_subgroup = joined_df_test\n",
    "                        else:\n",
    "                            joined_df_test_subgroup = joined_df_test[joined_df_test.subset == subgroup]\n",
    "                        print('Subgroup samples in test ', len(joined_df_test_subgroup))\n",
    "                        os.makedirs(f'/{output_dir}/test_plots/{subgroup}/', exist_ok=True)\n",
    "                        test_output_dir = f'/{output_dir}/test_plots/{subgroup}/{runcsv.replace(\".csv\", \"/\")}'\n",
    "                        os.makedirs(test_output_dir, exist_ok=True)\n",
    "                        test_setting = TestSetting(joined_df_test_subgroup,\n",
    "                                                   evaluated_score_column,\n",
    "                                                   ground_truth_column,\n",
    "                                                   test_output_dir)\n",
    "                        test_setting.set_current_expected_scenario(current_scenario)\n",
    "                        test_setting.initial_evaluation()\n",
    "\n",
    "\n",
    "                        # Fit calibrators with train subset\n",
    "                        for calibrator_name, calibrator in calibrators_dict.items():\n",
    "                            # Fit calibrators. For logReg, use effective_positive_priors to compute LLRs and to fit algorithm\n",
    "                            calibrator.train(scores=joined_df_train[evaluated_score_column].values,\n",
    "                                             labels=joined_df_train[ground_truth_column].values,\n",
    "                                             effective_positive_prior=0.5)\n",
    "\n",
    "                            # Evaluate effect on the same training set\n",
    "                            train_test_setting.set_current_expected_scenario(current_scenario)\n",
    "                            train_test_setting.calibration_evaluation(calibrator_name, calibrator)\n",
    "\n",
    "                            # Evaluate effect on the test set\n",
    "                            test_setting.set_current_expected_scenario(current_scenario)\n",
    "                            test_setting.calibration_evaluation(calibrator_name, calibrator, get_posteriors_thresholds=True)\n",
    "\n",
    "                        if subgroup == 'all':\n",
    "                            test_csv_dir = f'/{output_dir}/Runs_Test_with_calibration/{runcsv}'\n",
    "                            test_setting.df.to_csv(test_csv_dir)\n",
    "                            train_test_setting.results['N'] = [len(joined_df_train)] * len(train_test_setting.results)\n",
    "                            train_test_setting.results['N_positive'] = [len(\n",
    "                                joined_df_train[joined_df_train[ground_truth_column] == 1])] * len(\n",
    "                                train_test_setting.results)\n",
    "                            train_csv_dir = f'/{output_dir}/Runs_Validation_with_calibration/{runcsv}'\n",
    "                            train_test_setting.df.to_csv(train_csv_dir)\n",
    "                            all_train_results = all_train_results.append(train_test_setting.results, ignore_index=True)\n",
    "\n",
    "                            parameters[run].update(test_setting.calibrators_parameters)\n",
    "                        test_setting.results['subset'] = [subgroup]*len(test_setting.results)\n",
    "                        test_setting.results['N'] = [len(joined_df_test_subgroup)]*len(test_setting.results)\n",
    "                        test_setting.results['N_positive'] = [len(joined_df_test_subgroup[joined_df_test_subgroup[ground_truth_column] == 1])]*len(test_setting.results)\n",
    "                        all_test_results = all_test_results.append(test_setting.results, ignore_index=True)\n",
    "\n",
    "            all_test_results['run'] = [runcsv.replace('.csv', '')]*len(all_test_results)\n",
    "            if os.path.exists(f'/{output_dir}/test_metrics.csv'):\n",
    "                existing_results = pd.read_csv(f'/{output_dir}/test_metrics.csv')\n",
    "                existing_results = existing_results.append(all_test_results)\n",
    "                existing_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n",
    "            else:\n",
    "                all_test_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n",
    "\n",
    "            all_train_results['run'] = [runcsv.replace('.csv', '')]*len(all_train_results)\n",
    "            if os.path.exists(f'/{output_dir}/validation_metrics.csv'):\n",
    "                existing_results_train = pd.read_csv(f'/{output_dir}/validation_metrics.csv')\n",
    "                existing_results_train = existing_results_train.append(all_train_results)\n",
    "                existing_results_train.to_csv(f'/{output_dir}/validation_metrics.csv', index=False)\n",
    "            else:\n",
    "                all_train_results.to_csv(f'/{output_dir}/validation_metrics.csv', index=False)\n",
    "\n",
    "        try:\n",
    "            with open(f'/{output_dir}/parameters_results.json', \"w\") as fp:\n",
    "                json.dump(parameters, fp)\n",
    "        except Exception as e:\n",
    "            print('Error saving parameter json', e)\n",
    "\n",
    "        #Calculate deltas with perfect calibrated version\n",
    "        all_test_results = pd.read_csv(f'/{output_dir}/test_metrics.csv')\n",
    "        for run in all_test_results.run.unique():\n",
    "            print(run)\n",
    "            for subgroup in all_test_results.subset.unique():\n",
    "                perfect_run = all_test_results[(all_test_results.run == run) &\n",
    "                                               (all_test_results.calibrator == 'perfect_PAV') &\n",
    "                                               (all_test_results.subset == subgroup)]\n",
    "\n",
    "                for metric in ['CE', 'Balanced_CE', 'Brier', 'Balanced_Brier', 'predefined_cost']:\n",
    "                    perfect_metric = perfect_run[metric].values[0]\n",
    "                    for method in ['no_calibration', 'LOG-REG']:\n",
    "                        method_metric = all_test_results[(all_test_results.run == run) &\n",
    "                                                         (all_test_results.calibrator == method) &\n",
    "                                                         (all_test_results.subset == subgroup)][metric].values[0]\n",
    "                        all_test_results.at[(all_test_results['run'] == run) &\n",
    "                                            (all_test_results.calibrator == method) &\n",
    "                                            (all_test_results.subset == subgroup),\n",
    "                                            'delta_' + metric] = method_metric - perfect_metric\n",
    "        all_test_results.to_csv(f'/{output_dir}/test_metrics.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
